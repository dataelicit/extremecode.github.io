# Gradient Descent

Gradient Descent is one of the most used algorithms in Machine Learning and Deep Learning.

It is an optimization algorithm used in training a model. In simple words, Gradient Descent finds the parameters that minimize the cost function (error in prediction). Gradient Descent does this by iteratively moves toward a set of parameter values that minimize the function, taking steps in the opposite direction of the gradient

## Table of Contents 
* [Batch gradient descent](#Batch)
* [Stochastic gradient descent](#Stochastic)
* [Full gradient descent](#Full)
* [Momentum](#Momentum)
* [Full gradient descent](#Full)

<img src="./images/ann1.png" alt="data" class="inline"/>

## Batch gradient descent <a name="Batch"></a>


## Stochastic gradient descent <a name="Stochastic"></a>
<img src="./images/ann.png" alt="data" class="inline"/>

Stochastic gradient descent (SGD) performs a parameter update for each observation. So instead of looping over each observation, it just needs one to perform the parameter update. SGD is usually faster than batch gradient descent, but its frequent updates cause a higher variance in the error rate, that can sometimes jump around instead of decreasing.

## Full gradient descent <a name="Full"></a>

## Momentum <a name="Momentum"></a>
<img src="./images/ann2.png" alt="data" class="inline"/>

## Batch gradient descent <a name="Momentum"></a>
